{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IS-RSA+中介"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None) \n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "brain_path = '/txt/txt_7'\n",
    "\n",
    "data_dict = {}  \n",
    "\n",
    "for root, dirs, files in os.walk(brain_path):\n",
    "    for file in files:\n",
    "        if file.endswith('.txt'): \n",
    "            file_path = os.path.join(root, file)\n",
    "            with open(file_path, 'r') as f:\n",
    "                column_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "                matrix = np.loadtxt(file_path)\n",
    "                matrix = matrix[3:, :]\n",
    "                print(matrix.shape)\n",
    "        data_dict[column_name] = matrix\n",
    "\n",
    "brain_data = data_dict\n",
    "keys_list = list(data_dict.keys())\n",
    "print(keys_list)\n",
    "# brain_data.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "outpath = '/home/tjnu_fmri/wangqiang/lixiang/tmp_T1/GPT-T1/GPT_T1/RSA_GLM_3_TIV/results/1-12_single_cm/'\n",
    "beha_path = '/home/tjnu_fmri/wangqiang/lixiang/tmp_T1/GPT-T1/GPT_T1/RSA_GLM_3_TIV/results/1-4_mask_4/behave'\n",
    "\n",
    "all_beha = {}\n",
    "for filename in os.listdir(beha_path):\n",
    "    file_path = os.path.join(beha_path, filename)\n",
    "    df = pd.read_excel(file_path, index_col=0) \n",
    "    file_key = os.path.splitext(filename)[0]\n",
    "    all_beha[file_key] = df\n",
    "    \n",
    "beha_name = list(all_beha.keys()) \n",
    "print(beha_name)\n",
    "beha_name = [name for name in beha_name if name != 'GPT']\n",
    "print(len(beha_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def clean_data(a_column, behave, brain):\n",
    "    mask_behave = behave.notna()\n",
    "\n",
    "    valid_indices = mask_behave.all(axis=1)\n",
    "    cleaned_behave = behave[valid_indices]\n",
    "    cleaned_brain = brain[valid_indices]\n",
    "    cleaned_column = a_column[valid_indices]\n",
    "    \n",
    "    return cleaned_column, cleaned_behave, cleaned_brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def bootstrap_confint_indirect_effect(model_1, model_2, model_3, X, Y, Z, n_bootstrap=1000, ci=0.95):  \n",
    "    n = len(Y)\n",
    "    bootstrap_indirect_effects = []\n",
    "    bootstrap_1_coefs = []\n",
    "    bootstrap_2_coefs = []\n",
    "    bootstrap_3_coefs = []\n",
    "    \n",
    "    for _ in range(n_bootstrap):\n",
    "\n",
    "        sample_indices = np.random.choice(n, size=n, replace=True)\n",
    "        X_resampled = X.iloc[sample_indices]\n",
    "        Y_resampled = Y.iloc[sample_indices]\n",
    "        Z_resampled = Z.iloc[sample_indices]\n",
    "\n",
    "        model_1_resampled = sm.OLS(Y_resampled, sm.add_constant(X_resampled)).fit()\n",
    "        a_resampled = model_1_resampled.params[1]\n",
    "        bootstrap_1_coefs.append(model_1_resampled.params)\n",
    "        model_2_resampled = sm.OLS(Z_resampled, sm.add_constant(X_resampled)).fit()\n",
    "        bootstrap_2_coefs.append(model_2_resampled.params)\n",
    "        model_3_resampled = sm.OLS(Z_resampled, sm.add_constant(pd.concat([sm.add_constant(X_resampled), Y_resampled], axis=1))).fit()\n",
    "        b_resampled = model_3_resampled.params[0]\n",
    "        bootstrap_3_coefs.append(model_3_resampled.params)\n",
    "        indirect_effect_resampled = a_resampled * b_resampled\n",
    "        bootstrap_indirect_effects.append(indirect_effect_resampled)\n",
    "    \n",
    "    bootstrap_1_coefs = np.array(bootstrap_1_coefs)\n",
    "    bootstrap_2_coefs = np.array(bootstrap_2_coefs)\n",
    "    bootstrap_3_coefs = np.array(bootstrap_3_coefs)\n",
    "    lower_1_bound = np.percentile(bootstrap_1_coefs, (1 - ci) / 2 * 100, axis=0)\n",
    "    upper_1_bound = np.percentile(bootstrap_1_coefs, (1 + ci) / 2 * 100, axis=0)\n",
    "    lower_2_bound = np.percentile(bootstrap_2_coefs, (1 - ci) / 2 * 100, axis=0)\n",
    "    upper_2_bound = np.percentile(bootstrap_2_coefs, (1 + ci) / 2 * 100, axis=0)\n",
    "    lower_3_bound = np.percentile(bootstrap_3_coefs, (1 - ci) / 2 * 100, axis=0)\n",
    "    upper_3_bound = np.percentile(bootstrap_3_coefs, (1 + ci) / 2 * 100, axis=0)\n",
    "    ci_lower = np.percentile(bootstrap_indirect_effects, (1 - ci) / 2 * 100)\n",
    "    ci_upper = np.percentile(bootstrap_indirect_effects, (1 + ci) / 2 * 100)\n",
    "    return lower_2_bound, upper_2_bound, ci_lower, ci_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "def clean_row(row):\n",
    "    for column, value in row.items(): \n",
    "        if isinstance(value, str) or value > 1000:\n",
    "            row[column] = np.nan\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_correlation(y):\n",
    "    R = 1 - np.corrcoef(y)\n",
    "    R_1 = np.tril(R)\n",
    "    R_1[np.triu_indices_from(R_1, 0)] = 99999\n",
    "    R_1 = R_1.T\n",
    "    return R_1[R_1 < 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def calculate_distances(data_matrix):\n",
    "    distances = []\n",
    "    for i in range(len(data_matrix)):\n",
    "        for j in range(i + 1, len(data_matrix)):\n",
    "            # distance = np.sqrt(np.sum((data_matrix[i, :] - data_matrix[j, :]) ** 2))#NumPy ：data_matrix[i, :]\n",
    "            distance = np.sqrt(np.sum((data_matrix.iloc[i, :] - data_matrix.iloc[j, :]) ** 2))#N DataFrame \n",
    "            distances.append(distance)\n",
    "    return np.array(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import pearsonr\n",
    "from statsmodels.stats.mediation import Mediation\n",
    "\n",
    "GPT_column = pd.DataFrame(all_beha['GPT'])\n",
    "R_P = []\n",
    "results = []\n",
    "\n",
    "for beha_column_n in beha_name:\n",
    "    RSA = []\n",
    "    beha_column = all_beha[beha_column_n]\n",
    "    beha_column = beha_column.apply(clean_row, axis=1)\n",
    "    beha_column = pd.DataFrame(beha_column)\n",
    "    mask_behave = beha_column.notna()\n",
    "    valid_indices = mask_behave.all(axis=1)\n",
    "    cleaned_behave = beha_column[valid_indices]\n",
    "    cleaned_GPT = GPT_column[valid_indices]\n",
    "    print(cleaned_behave.shape)\n",
    "    print(cleaned_GPT.shape)\n",
    "    cleaned_behave_RSA = calculate_distances(cleaned_behave)\n",
    "    cleaned_GPT_RSA = calculate_distances(cleaned_GPT)\n",
    "    \n",
    "    RSA.append({\n",
    "        'beha_column_RSA': cleaned_behave_RSA,\n",
    "        'GPT_column_RSA': cleaned_GPT_RSA\n",
    "    })\n",
    "    file_path_3 = os.path.join(outpath, f'{beha_column_n}',f'col_{beha_column_n}.csv')\n",
    "    cleaned_behave = pd.DataFrame(cleaned_behave)\n",
    "    cleaned_behave.to_csv(file_path_3, sep='\\t', index=False)\n",
    "    file_path_4 = os.path.join(outpath, f'{beha_column_n}',f'RSA_{beha_column_n}.csv')\n",
    "    cleaned_behave_RSA_df = pd.DataFrame(cleaned_behave_RSA)\n",
    "    cleaned_behave_RSA_df.to_csv(file_path_4, sep='\\t', index=False)\n",
    "    file_path_5 = os.path.join(outpath, f'{beha_column_n}',f'col_GPT_{beha_column_n}.csv')\n",
    "    cleaned_GPT = pd.DataFrame(cleaned_GPT)\n",
    "    cleaned_GPT.to_csv(file_path_5, sep='\\t', index=False)\n",
    "    file_path_6 = os.path.join(outpath, f'{beha_column_n}',f'RSA_GPT_{beha_column_n}.csv')\n",
    "    cleaned_GPT_RSA_df = pd.DataFrame(cleaned_GPT_RSA)\n",
    "    cleaned_GPT_RSA_df.to_csv(file_path_6, sep='\\t', index=False)\n",
    "    \n",
    "    for brain_column_n in keys_list:\n",
    "        brain_column = brain_data[brain_column_n]\n",
    "        # brain_column = pd.DataFrame(brain_column)\n",
    "        cleaned_brain = brain_column[valid_indices]\n",
    "        sub_num = cleaned_GPT.shape[0]\n",
    "        print(cleaned_brain.shape)\n",
    "\n",
    "        file_path = os.path.join(outpath, f'{beha_column_n}')\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "        file_path_1= os.path.join(outpath, f'{beha_column_n}',f'col_{brain_column_n}.csv')\n",
    "        cleaned_brain_df = pd.DataFrame(cleaned_brain)\n",
    "        cleaned_brain_df.to_csv(file_path_1, sep='\\t', index=False)\n",
    "\n",
    "        cleaned_brain = calculate_correlation(cleaned_brain)\n",
    "        print(cleaned_brain.shape)\n",
    "        RSA[0][brain_column_n] = cleaned_brain\n",
    "        file_path_2= os.path.join(outpath, f'{beha_column_n}',f'RSA_{brain_column_n}.csv')\n",
    "        cleaned_brain_df = pd.DataFrame(cleaned_brain)\n",
    "        cleaned_brain_df.to_csv(file_path_2, sep='\\t', index=False)\n",
    "        \n",
    "        corr_brain_behave, p_brain_behave = pearsonr(cleaned_brain, cleaned_behave_RSA)\n",
    "        corr_brain_GPT, p_brain_GPT = pearsonr(cleaned_brain, cleaned_GPT_RSA)\n",
    "        corr_behave_GPT, p_behave_GPT = pearsonr(cleaned_behave_RSA, cleaned_GPT_RSA)\n",
    "        \n",
    "        R_P.append({\n",
    "            '自变量': \"GPT\",\n",
    "            '中介变量': brain_column_n,\n",
    "            '因变量': beha_column_n,\n",
    "            'corr_brain_behave': corr_brain_behave,\n",
    "            'p_brain_behave': p_brain_behave,\n",
    "            'corr_brain_GPT': corr_brain_GPT,\n",
    "            'p_brain_GPT': p_brain_GPT,\n",
    "            'corr_behave_GPT': corr_behave_GPT,\n",
    "            'p_behave_GPT': p_behave_GPT\n",
    "        })\n",
    "        RP_print = pd.DataFrame(R_P)\n",
    "        print(RP_print)\n",
    "        \n",
    "        alpha = 0.05 \n",
    "        if p_brain_behave < alpha and p_brain_GPT < alpha and p_behave_GPT < alpha and corr_brain_behave > 0 and corr_brain_GPT > 0 and corr_behave_GPT > 0:\n",
    "\n",
    "            print(\"All correlations are significant. Performing Operation CM.\")\n",
    "        \n",
    "            x = cleaned_GPT_RSA.astype(float)\n",
    "            X = sm.add_constant(x) \n",
    "            Y = cleaned_brain.astype(float) \n",
    "            print(X.shape)\n",
    "            mediation_model_1 = sm.OLS(Y, X).fit()\n",
    "            Z = cleaned_behave_RSA.astype(float)\n",
    "            mediation_model_2 = sm.OLS(Z, X).fit()\n",
    "            X = pd.DataFrame(X)\n",
    "            Y = pd.DataFrame(Y)\n",
    "            Z = pd.DataFrame(Z)\n",
    "            mediation_model_3 = sm.OLS(Z, sm.add_constant(pd.concat([X.iloc[:, 1:], Y], axis=1))).fit()\n",
    "\n",
    "            indirect_effect = mediation_model_1.params[1] * mediation_model_3.params[0] \n",
    "            total_effect = mediation_model_3.params[1] + indirect_effect\n",
    "\n",
    "            std_x = np.std(X.iloc[:, 1:].values)\n",
    "            std_y = np.std(Y.values)\n",
    "            std_z = np.std(Z.values)\n",
    "            std_indirect_effect = indirect_effect * std_x / std_z\n",
    "            std_total_effect = total_effect * std_x / std_z \n",
    "\n",
    "            ci_lower, ci_upper, indirect_effect_lower, indirect_effect_upper = bootstrap_confint_indirect_effect(\n",
    "            mediation_model_1, mediation_model_2, mediation_model_3, X[1], Y, Z, n_bootstrap=1000, ci=0.95)\n",
    "\n",
    "            results.append({\n",
    "                '自变量': \"GPT\",\n",
    "                '中介变量': brain_column_n,\n",
    "                '因变量': beha_column_n,\n",
    "                '被试数量': sub_num,\n",
    "                '非标准化系数（X->M）': mediation_model_1.params[1],\n",
    "                'P值（X->M）': mediation_model_1.pvalues[1],\n",
    "                '非标准化系数（X->Y）': mediation_model_3.params[1],\n",
    "                'P值（X->Y）': mediation_model_3.pvalues[1],\n",
    "                '非标准化系数（M->Y）': mediation_model_3.params[0],\n",
    "                'P值（M->Y）': mediation_model_3.pvalues[0],\n",
    "                '标准化系数（X->M）': mediation_model_1.params[1] * std_x/std_y,\n",
    "                '标准化系数（X->Y）': mediation_model_3.params[1] * std_x/std_z,\n",
    "                '标准化系数（M->Y）': mediation_model_3.params[0] * std_y/ std_z,\n",
    "                '非标准化间接效应': indirect_effect,\n",
    "                '标准化间接效应': std_indirect_effect,\n",
    "                'indir置信区间下限': indirect_effect_lower,\n",
    "                'indir置信区间上限': indirect_effect_upper,\n",
    "                '非标准化总效应': total_effect,\n",
    "                '标准化总效应': std_total_effect,\n",
    "                'tota置信区间下限': ci_lower[1],\n",
    "                'tota置信区间上限': ci_upper[1]\n",
    "            })\n",
    "            results_print = pd.DataFrame(results)\n",
    "            print(results_print)\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "    beha_result = results_print\n",
    "    beha_result_path = os.path.join(outpath, f'{beha_column_n}.csv')\n",
    "    beha_result.to_csv(beha_result_path, sep='\\t', index=False)\n",
    "    rsa_df = pd.DataFrame(RSA)\n",
    "    rsa_file_path = os.path.join(outpath, f'{beha_column_n}', f'RSA_all_{beha_column_n}.csv')\n",
    "    rsa_df.to_csv(rsa_file_path, sep='\\t', index=False)\n",
    "    file_path_7 = os.path.join(outpath, f'{beha_column_n}','R_P.csv')\n",
    "    RP_print.to_csv(file_path_7, sep='\\t', index=False)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "RP_df = pd.DataFrame(R_P)\n",
    "print(RP_df)\n",
    "results_df_path = os.path.join(outpath, 'RSA_cm_results.csv')\n",
    "results_df.to_csv(results_df_path, sep='\\t', index=False)\n",
    "RP_df_path = os.path.join(outpath, 'RSA_rp_results.csv')\n",
    "RP_df.to_csv(RP_df_path, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
